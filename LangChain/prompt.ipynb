{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from api import API_KEYS\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEYS[\"OPENAI\"]\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = API_KEYS[\"HUGGINFACE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API CALLING TEST\n",
    "\n",
    "### 프롬포트 생성\n",
    "\n",
    "- PromptTemplate은 LLM에 문장을 전달하기 전에 문장 구성을 편리하게 만들어주는 역할\n",
    "- LLM이 어떤 문장을 만들어야 하는지를 알려주는 역할을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'카메라를 홍보하기 위한 좋은 문구를 추천해줘?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"{product}를 홍보하기 위한 좋은 문구를 추천해줘?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "prompt.format(product=\"카메라\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "거북이입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm1 = ChatOpenAI(temperature=0,\n",
    "                  model_name='gpt-4')\n",
    "\n",
    "prompt = \"상범이는 거북이를 키우고 있습니다. 상범이가 키우고 있는 동물은?\"\n",
    "\n",
    "print(llm1.predict(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='거북이입니다.' response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 42, 'total_tokens': 48}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-67cfc353-7d0c-40ba-981b-2d1d23d63fb8-0' usage_metadata={'input_tokens': 42, 'output_tokens': 6, 'total_tokens': 48}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm1 = ChatOpenAI(temperature=0,\n",
    "                  model_name='gpt-4')\n",
    "\n",
    "prompt = \"상범이는 거북이를 키우고 있습니다. 상범이가 키우고 있는 동물은?\"\n",
    "\n",
    "print(llm1.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUGGING FACE\n",
    "- 시간마다 request할 수 있는 양의 한계가 있음\n",
    "- google/flan-t5-xxl 모델 사용시, 바로 request 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         \n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm2 = HuggingFaceHub(repo_id = \"google/flan-t5-small\", \n",
    "                      model_kwargs={\"temperature\":0.8})\n",
    "\n",
    "completion = llm2(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hugging Face Model은 제대로 대답을 하지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "대한민국의 가을은 몇 월부터 몇 월까지야?\n",
      "\n",
      "client=<openai.resources.chat.completions.Completions object at 0x10c612c50> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10c6283a0> model_name='gpt-4' temperature=0.0 openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/pydantic/v1/main.py:996: RuntimeWarning: fields may not start with an underscore, ignoring \"_input\"\n",
      "  warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m대한민국의 가을은 일반적으로 9월부터 11월까지입니다.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-small', 'task': 'text2text-generation', 'model_kwargs': {'temperature': 0.8}}\n",
      "\u001b[33;1m\u001b[1;3m         \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.model_laboratory import ModelLaboratory\n",
    "model_lab = ModelLaboratory.from_llms([llm1, llm2])\n",
    "model_lab.compare(\"대한민국의 가을은 몇 월부터 몇 월까지야?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 max_tokens=2048,\n",
    "                 model_name='gpt-4',\n",
    "                 )\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"7개의 팀을 보여줘 {subject}.format\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['두산 베어스', '롯데 자이언츠', '삼성 라이온즈', 'SK 와이번스', 'KIA 타이거즈', 'LG 트윈스', 'NC 다이노스']\n"
     ]
    }
   ],
   "source": [
    "query = \"한국의 야구팀은?\"\n",
    "\n",
    "output = llm.predict(text=prompt.format(subject=query))\n",
    "\n",
    "# 출력의 format 변경\n",
    "parsed_results = output_parser.parse(output)\n",
    "print(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1    The Fence \\n \\nTom Sawyer lived with his aunt because his mother and \\nfather were dead. Tom didn’t like going to school, and he didn’t like working. He liked playing and having adventures. One Friday, he didn’t go to school—he went to the river. \\nAunt Polly was angry. “You’re a bad boy!” she said. \\n“Tomorrow you can’t play with your friends because you didn’t go to school today. Tomorrow you’re going to work for me. You can paint the fence.” \\nSaturday morning, Tom was not happy, but he started to \\npaint the fence. His friend Jim was in the street. \\nTom asked him, “Do you want to paint?” \\nJim said, “No, I can’t. I’m going to get water.” \\nThen Ben came to Tom’s house. He watched Tom and \\nsaid, “I’m going to swim today. You can’t swim because you’re working.” \\nTom said, “This isn’t work. I like painting.” \\n“Can I paint, too?” Ben asked. \\n“No, you can’t,” Tom answered. “Aunt Polly asked me \\nbecause I’m a very good painter.” \\nBen said, “I’m a good painter, too. Please, can I paint? I \\nhave some fruit. Do you want it?” \\nOK,” Tom said. “Give me the fruit. Then you can paint.” \\nBen started to paint the fence. Later, many boys came to \\nTom’s house. They watched Ben, and they wanted to paint, too. \\nTom said, “Give me some food and you can paint.” \\n \\n1 '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"The_Adventures_of_Tom_Sawyer.pdf\")\n",
    "document = loader.load()\n",
    "document[5].page_content[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(document, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마을 무덤에 있던 남자를 죽인 사람은 Injun Joe입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name = 'gpt-3.5-turbo',\n",
    "                 )\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "query = \"마을 무덤에 있던 남자를 죽인 사람은 누구니?\"\n",
    "result = qa({'query': query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lang Chain\n",
    "\n",
    "- 여러개의 모델을 하나로 연결하는 것\n",
    "\n",
    "### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'프랑스의 수도는 파리입니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name='gpt-4',\n",
    "                 )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"{country}의 수도는 어디야?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"프랑스\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['sentence'],\n",
    "    template = \"다음 문장을 한글로 번역하세요. \\n\\n {sentence}\"\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key='translation')\n",
    "\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "    \"다음 문장을 네 문장으로 요약하세요. \\n\\n {sentence}\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key='summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "all_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=['sentence'],\n",
    "    output_variables=['translation', 'summary'],\n",
    ")\n",
    "\n",
    "sentence=\"\"\"\n",
    "    The YOLOv10 paper introduces a new real-time end-to-end object detector that significantly advances the performance-efficiency boundary of the YOLO series. \n",
    "    The authors address the reliance on non-maximum suppression (NMS) for post-processing, which hinders end-to-end deployment and impacts inference latency. \n",
    "    They propose a consistent dual assignments strategy for NMS-free training, improving both performance and latency. Additionally, the paper introduces \n",
    "    a holistic efficiency-accuracy driven model design strategy, optimizing various components to reduce computational overhead and enhance capability. \n",
    "    Extensive experiments demonstrate that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales, \n",
    "    outperforming previous models in terms of computation-accuracy trade-offs.\n",
    "\"\"\"\n",
    "\n",
    "ans = all_chain(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '\\n    The YOLOv10 paper introduces a new real-time end-to-end object detector that significantly advances the performance-efficiency boundary of the YOLO series. \\n    The authors address the reliance on non-maximum suppression (NMS) for post-processing, which hinders end-to-end deployment and impacts inference latency. \\n    They propose a consistent dual assignments strategy for NMS-free training, improving both performance and latency. Additionally, the paper introduces \\n    a holistic efficiency-accuracy driven model design strategy, optimizing various components to reduce computational overhead and enhance capability. \\n    Extensive experiments demonstrate that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales, \\n    outperforming previous models in terms of computation-accuracy trade-offs.\\n',\n",
       " 'translation': 'YOLOv10 논문은 YOLO 시리즈의 성능-효율성 경계를 크게 향상시키는 새로운 실시간 종단간 객체 탐지기를 소개합니다. \\n저자들은 후처리를 위한 최대 비최대 억제(non-maximum suppression, NMS)에 대한 의존성을 다루며, 이는 종단간 배포를 방해하고 추론 지연에 영향을 미칩니다. \\n그들은 NMS-free 훈련을 위한 일관된 이중 할당 전략을 제안하며, 이는 성능과 지연 시간 모두를 향상시킵니다. 또한, 이 논문은 다양한 구성 요소를 최적화하여 \\n계산 오버헤드를 줄이고 능력을 향상시키는 종합적인 효율성-정확도 중심의 모델 설계 전략을 소개합니다. \\n다양한 모델 규모에서의 광범위한 실험은 YOLOv10이 최첨단 성능과 효율성을 달성하며, 계산-정확도 트레이드오프 측면에서 이전 모델들을 능가함을 보여줍니다.',\n",
       " 'summary': 'The YOLOv10 paper presents a new real-time object detector that improves the performance-efficiency of the YOLO series. The authors tackle the issue of reliance on non-maximum suppression (NMS) which affects end-to-end deployment and inference latency. They suggest a dual assignments strategy for NMS-free training, enhancing both performance and latency. The paper also introduces a comprehensive model design strategy, optimizing different components to lower computational overhead and increase capability.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The YOLOv10 paper presents a new real-time object detector that improves the performance-efficiency of the YOLO series. The authors tackle the issue of reliance on non-maximum suppression (NMS) which affects end-to-end deployment and inference latency. They suggest a dual assignments strategy for NMS-free training, enhancing both performance and latency. The paper also introduces a comprehensive model design strategy, optimizing different components to lower computational overhead and increase capability.\n"
     ]
    }
   ],
   "source": [
    "print(ans['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv10 논문은 YOLO 시리즈의 성능-효율성 경계를 크게 향상시키는 새로운 실시간 종단간 객체 탐지기를 소개합니다. \n",
      "저자들은 후처리를 위한 최대 비최대 억제(non-maximum suppression, NMS)에 대한 의존성을 다루며, 이는 종단간 배포를 방해하고 추론 지연에 영향을 미칩니다. \n",
      "그들은 NMS-free 훈련을 위한 일관된 이중 할당 전략을 제안하며, 이는 성능과 지연 시간 모두를 향상시킵니다. 또한, 이 논문은 다양한 구성 요소를 최적화하여 \n",
      "계산 오버헤드를 줄이고 능력을 향상시키는 종합적인 효율성-정확도 중심의 모델 설계 전략을 소개합니다. \n",
      "다양한 모델 규모에서의 광범위한 실험은 YOLOv10이 최첨단 성능과 효율성을 달성하며, 계산-정확도 트레이드오프 측면에서 이전 모델들을 능가함을 보여줍니다.\n"
     ]
    }
   ],
   "source": [
    "print(ans[\"translation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 상범이는 거북이입니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 상범이는 거북이입니다.\n",
      "AI: 그렇군요, 상범이가 거북이라는 것을 알게 되어 반가워요. 거북이는 느린 움직임과 오래 살 수 있는 것으로 잘 알려져 있어요. 상범이는 어떤 종류의 거북이인가요? 그리고 상범이의 특별한 습성이나 특징이 있다면 알려주세요.\n",
      "Human: 상범이는 1년 뒤에 30%의 확률로 암컷이 됩니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 상범이는 거북이입니다.\n",
      "AI: 그렇군요, 상범이가 거북이라는 것을 알게 되어 반가워요. 거북이는 느린 움직임과 오래 살 수 있는 것으로 잘 알려져 있어요. 상범이는 어떤 종류의 거북이인가요? 그리고 상범이의 특별한 습성이나 특징이 있다면 알려주세요.\n",
      "Human: 상범이는 1년 뒤에 30%의 확률로 암컷이 됩니다.\n",
      "AI: 상범이가 성별 결정에 확률적 요소가 있는 거북이 종류라는 것이 흥미롭네요. 일반적으로 거북이의 성별은 부화 시점의 온도에 따라 결정되는데, 상범이의 종류는 다른 방식으로 성별이 결정되는 것 같아요. 상범이가 암컷이 될 확률이 30%라면, 그 외의 확률로 수컷이 될 가능성도 있을까요?\n",
      "Human: 대호는 상범이를 세 마리 키웁니다.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 상범이는 거북이입니다.\n",
      "AI: 그렇군요, 상범이가 거북이라는 것을 알게 되어 반가워요. 거북이는 느린 움직임과 오래 살 수 있는 것으로 잘 알려져 있어요. 상범이는 어떤 종류의 거북이인가요? 그리고 상범이의 특별한 습성이나 특징이 있다면 알려주세요.\n",
      "Human: 상범이는 1년 뒤에 30%의 확률로 암컷이 됩니다.\n",
      "AI: 상범이가 성별 결정에 확률적 요소가 있는 거북이 종류라는 것이 흥미롭네요. 일반적으로 거북이의 성별은 부화 시점의 온도에 따라 결정되는데, 상범이의 종류는 다른 방식으로 성별이 결정되는 것 같아요. 상범이가 암컷이 될 확률이 30%라면, 그 외의 확률로 수컷이 될 가능성도 있을까요?\n",
      "Human: 대호는 상범이를 세 마리 키웁니다.\n",
      "AI: 대호님이 상범이라는 이름의 거북이를 세 마리나 키우고 계시다니 대단하네요! 각각의 거북이들이 서로 다른 성격이나 특징을 가지고 있을 수 있어요. 대호님께서는 이들과 어떻게 상호작용하시는지, 그리고 이들을 키우는 데 있어 특별히 주의하시는 점이 있다면 공유해주실 수 있을까요?\n",
      "Human: 대호가 키우는 상범이가 1년뒤에 모두 암컷일 확률은?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'상범이가 암컷이 될 확률이 30%라고 했을 때, 대호님이 키우는 세 마리의 상범이가 모두 암컷이 될 확률은 각각의 확률을 곱하는 것으로 계산할 수 있어요. 따라서 0.3 (30%) * 0.3 (30%) * 0.3 (30%) = 0.027, 즉 약 2.7%의 확률로 모두 암컷이 될 수 있습니다. 이는 매우 낮은 확률이지만, 불가능한 일은 아니에요.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name='gpt-4'\n",
    "                 )\n",
    "\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"상범이는 거북이입니다.\")\n",
    "conversation.predict(input=\"상범이는 1년 뒤에 30%의 확률로 암컷이 됩니다.\")\n",
    "conversation.predict(input=\"대호는 상범이를 세 마리 키웁니다.\")\n",
    "conversation.predict(input=\"대호가 키우는 상범이가 1년뒤에 모두 암컷일 확률은?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m먼저 아주대학교의 특징에 대해 알아보기 위해 위키백과를 사용하겠습니다. 그리고 아주대학교의 개교 연도를 알아낸 후 2024년과의 차이를 계산하여 개교 후 몇 년이 되는지 알아보겠습니다.\n",
      "Action: wikipedia\n",
      "Action Input: 아주대학교\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Ajou University\n",
      "Summary: Ajou University (Korean: 아주대학교; Hanja: 亞洲大學校; lit. Asia University) is a private research university located in Suwon, Gyeonggi Province, South Korea. It was founded in 1973 by Park Changwon.\n",
      "Ajou University started as Ajou Engineering Junior College with the aim of exchanging technology and culture through education between France and South Korea.\n",
      "Ajou is located on a single campus witfh all details, research-oriented university hospitals, Techno Valley, Samsung Electronics headquarters, CJ Blossom Park, administrative town (Gyeonggi Provincial Office, Gyeonggi Provincial Council, Education Office, Gyeonggi Representative Library), and legal town (Suwon Court, Suwon Prosecutor's Office). Gyeonggi-do Pharmaceutical Association has signed MOUs with Ajou.\n",
      "Its business college is the only one in Korea that was authorized to host a financial engineering department as part of the world-class research-oriented university project of the Korean Ministry of Education and held a recruitment briefing session for the International Financial Organization. Its business college is a member of both the AACSB and EFMD certification bodies. \n",
      "The e-business department signed an education agreement with SAP Korea. \n",
      "Ajou University Hospital, an affiliated hospital of the medical school and a base hospital in Gyeonggi-do, is opening Ajou University Intermediate Care Hospital, Ajou University Pyeongtaek Hospital, Ajou University Paju Hospital. It received a hospital brand fee from Jiya Yoon Hospital in China. \n",
      "In 2020, Ajou established a campus in Tashkent, Uzbekistan which boasts strong engineering programs.\n",
      "\n",
      "Page: Ahn Jung-hwan\n",
      "Summary: Ahn Jung-hwan (Korean: 안정환; RR: An Jeong-hwan, Korean pronunciation: [an.dʑʌŋ.ɦwan] or [an] [tɕʌŋ.ɦwan]; born 27 January 1976) is a South Korean television personality and former professional footballer. A versatile forward known for his technical skills and clutch goalscoring, Ahn represented South Korea in three FIFA World Cups, notably scoring a golden goal against Italy in 2002. Following his retirement, Ahn transitioned into a successful career in broadcasting, becoming a popular football commentator and television host. He is also recognized for his philanthropic endeavors.\n",
      "\n",
      "Page: Ajou Law School\n",
      "Summary: Ajou Law School (아주대학교 법학전문대학원) is one of the professional graduate schools of Ajou University, located in Suwon, South Korea. Founded in 2009, it is one of the founding law schools in South Korea and is one of the smaller schools with each class in the three-year J.D. program having approximately 50 students.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m아주대학교는 1973년에 설립되었으며, 프랑스와 한국 간의 교육을 통한 기술과 문화 교류를 목표로 시작한 곳입니다. 이는 한 캠퍼스에 위치해 있으며, 연구 중심의 대학 병원, 테크노 밸리, 삼성전자 본사, CJ 블로썸 파크, 행정 도시(경기도청, 경기도의회, 교육청, 경기대표 도서관), 법률 도시(수원 법원, 수원 검찰청) 등이 있습니다. 또한, 경기도 약사회와 MOU를 체결하였습니다. 그리고 그들의 사업 대학은 한국 교육부의 세계 수준의 연구 중심 대학 프로젝트의 일환으로 금융 공학 부서를 개설할 권한을 받은 한국에서 유일한 곳이며, AACSB와 EFMD 인증 기관의 회원입니다. 이제 2024년과 1973년의 차이를 계산해야 합니다.\n",
      "Action: Calculator\n",
      "Action Input: 2024 - 1973\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 51\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m아주대학교는 1973년에 설립되었으므로, 2024년에는 개교 51주년을 맞이하게 됩니다.\n",
      "Final Answer: 아주대학교는 1973년에 설립되었으며, 프랑스와 한국 간의 교육을 통한 기술과 문화 교류를 목표로 시작한 곳입니다. 이는 한 캠퍼스에 위치해 있으며, 연구 중심의 대학 병원, 테크노 밸리, 삼성전자 본사, CJ 블로썸 파크, 행정 도시(경기도청, 경기도의회, 교육청, 경기대표 도서관), 법률 도시(수원 법원, 수원 검찰청) 등이 있습니다. 또한, 경기도 약사회와 MOU를 체결하였습니다. 그리고 그들의 사업 대학은 한국 교육부의 세계 수준의 연구 중심 대학 프로젝트의 일환으로 금융 공학 부서를 개설할 권한을 받은 한국에서 유일한 곳이며, AACSB와 EFMD 인증 기관의 회원입니다. 2024년에는 개교 51주년을 맞이하게 됩니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'아주대학교는 1973년에 설립되었으며, 프랑스와 한국 간의 교육을 통한 기술과 문화 교류를 목표로 시작한 곳입니다. 이는 한 캠퍼스에 위치해 있으며, 연구 중심의 대학 병원, 테크노 밸리, 삼성전자 본사, CJ 블로썸 파크, 행정 도시(경기도청, 경기도의회, 교육청, 경기대표 도서관), 법률 도시(수원 법원, 수원 검찰청) 등이 있습니다. 또한, 경기도 약사회와 MOU를 체결하였습니다. 그리고 그들의 사업 대학은 한국 교육부의 세계 수준의 연구 중심 대학 프로젝트의 일환으로 금융 공학 부서를 개설할 권한을 받은 한국에서 유일한 곳이며, AACSB와 EFMD 인증 기관의 회원입니다. 2024년에는 개교 51주년을 맞이하게 됩니다.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name='gpt-4'\n",
    "                 )\n",
    "\n",
    "tools = load_tools(['wikipedia', 'llm-math'], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools,\n",
    "                         llm,\n",
    "                         agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                         description='계산이 필요할 때 사용',\n",
    "                         verbose=True\n",
    "                         )\n",
    "\n",
    "agent.run(\"아주대학교의 특징은? 2024년도 현재 올해 개교 후 몇 년?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "documents = TextLoader(\"AI.txt\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "docs = split_docs(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name = 'all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model_name = 'gpt-3.5-turbo'\n",
    "llm = ChatOpenAI(model_name = model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. It is a field of study in computer science that develops and studies intelligent machines. Such machines may be called AIs.\n",
      "\n",
      "AI technology is widely used throughout industry, government, and science. Some high-profile applications are: advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Google Assistant, Siri, and Alexa), self-driving cars (e.g., Waymo), generative and creative tools (ChatGPT and AI art), and superhuman play and analysis in strategy games (such as chess and Go).[1]\n",
      "\n",
      "The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics.[a] General intelligence (the ability to complete any task performable by a human) is among the field's long-term goals.[11]\n",
      "\n",
      "To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience and other fields.[12]\n",
      "\n",
      "Alan Turing was the first person to carry out substantial research in the field that he called Machine Intelligence.[2] Artificial intelligence was founded as an academic discipline in 1956.[3] The field went through multiple cycles of optimism[4][5] followed by disappointment and loss of funding.[6][7] Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques,[8] and after 2017 with the transformer architecture.[9] This led to the AI spring of the 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.[10]\n",
      "Human: AI란?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type='stuff', verbose=True)\n",
    "\n",
    "query = 'AI란?'\n",
    "matching_docs = db.similarity_search(query)\n",
    "ans = chain.run(input_documents=matching_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI(인공지능)는 인간이나 동물의 지능과는 달리 기계나 소프트웨어의 지능을 의미합니다.\n",
      " 컴퓨터과학의 한 분야로, 지능적인 기계를 개발하고 연구하는 분야입니다.\n",
      " AI 기술은 산업, 정부, 과학 분야에서 널리 사용되며, 구글 검색 엔진, YouTube, Amazon, Netflix의 추천 시스템, 구글 어시스턴트, Siri, Alexa와 같은 음성인식 기술, 웨이모와 같은 자율주행 자동차, ChatGPT 및 AI 아트와 같은 창의적인 도구, 그리고 체스와 바둑과 같은 전략 게임에서의 초인간적인 플레이와 분석 등이 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ans.replace('.', '.\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
