{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from api import API_KEYS\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = API_KEYS[\"OPENAI\"]\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = API_KEYS[\"HUGGINFACE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API CALLING TEST\n",
    "\n",
    "### 프롬포트 생성\n",
    "\n",
    "- PromptTemplate은 LLM에 문장을 전달하기 전에 문장 구성을 편리하게 만들어주는 역할\n",
    "- LLM이 어떤 문장을 만들어야 하는지를 알려주는 역할을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'카메라를 홍보하기 위한 좋은 문구를 추천해줘?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"{product}를 홍보하기 위한 좋은 문구를 추천해줘?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "prompt.format(product=\"카메라\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "거북이입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm1 = ChatOpenAI(temperature=0,\n",
    "                  model_name='gpt-4')\n",
    "\n",
    "prompt = \"상범이는 거북이를 키우고 있습니다. 상범이가 키우고 있는 동물은?\"\n",
    "\n",
    "print(llm1.predict(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='거북이입니다.' response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 42, 'total_tokens': 48}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-67cfc353-7d0c-40ba-981b-2d1d23d63fb8-0' usage_metadata={'input_tokens': 42, 'output_tokens': 6, 'total_tokens': 48}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm1 = ChatOpenAI(temperature=0,\n",
    "                  model_name='gpt-4')\n",
    "\n",
    "prompt = \"상범이는 거북이를 키우고 있습니다. 상범이가 키우고 있는 동물은?\"\n",
    "\n",
    "print(llm1.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUGGING FACE\n",
    "- 시간마다 request할 수 있는 양의 한계가 있음\n",
    "- google/flan-t5-xxl 모델 사용시, 바로 request 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         \n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "llm2 = HuggingFaceHub(repo_id = \"google/flan-t5-small\", \n",
    "                      model_kwargs={\"temperature\":0.8})\n",
    "\n",
    "completion = llm2(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hugging Face Model은 제대로 대답을 하지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mInput:\u001b[0m\n",
      "대한민국의 가을은 몇 월부터 몇 월까지야?\n",
      "\n",
      "client=<openai.resources.chat.completions.Completions object at 0x10c612c50> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x10c6283a0> model_name='gpt-4' temperature=0.0 openai_api_key=SecretStr('**********') openai_proxy=''\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/pydantic/v1/main.py:996: RuntimeWarning: fields may not start with an underscore, ignoring \"_input\"\n",
      "  warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m대한민국의 가을은 일반적으로 9월부터 11월까지입니다.\u001b[0m\n",
      "\n",
      "\u001b[1mHuggingFaceHub\u001b[0m\n",
      "Params: {'repo_id': 'google/flan-t5-small', 'task': 'text2text-generation', 'model_kwargs': {'temperature': 0.8}}\n",
      "\u001b[33;1m\u001b[1;3m         \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.model_laboratory import ModelLaboratory\n",
    "model_lab = ModelLaboratory.from_llms([llm1, llm2])\n",
    "model_lab.compare(\"대한민국의 가을은 몇 월부터 몇 월까지야?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 max_tokens=2048,\n",
    "                 model_name='gpt-4',\n",
    "                 )\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"7개의 팀을 보여줘 {subject}.format\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['두산 베어스', '롯데 자이언츠', '삼성 라이온즈', 'SK 와이번스', 'KIA 타이거즈', 'LG 트윈스', 'NC 다이노스']\n"
     ]
    }
   ],
   "source": [
    "query = \"한국의 야구팀은?\"\n",
    "\n",
    "output = llm.predict(text=prompt.format(subject=query))\n",
    "\n",
    "# 출력의 format 변경\n",
    "parsed_results = output_parser.parse(output)\n",
    "print(parsed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1    The Fence \\n \\nTom Sawyer lived with his aunt because his mother and \\nfather were dead. Tom didn’t like going to school, and he didn’t like working. He liked playing and having adventures. One Friday, he didn’t go to school—he went to the river. \\nAunt Polly was angry. “You’re a bad boy!” she said. \\n“Tomorrow you can’t play with your friends because you didn’t go to school today. Tomorrow you’re going to work for me. You can paint the fence.” \\nSaturday morning, Tom was not happy, but he started to \\npaint the fence. His friend Jim was in the street. \\nTom asked him, “Do you want to paint?” \\nJim said, “No, I can’t. I’m going to get water.” \\nThen Ben came to Tom’s house. He watched Tom and \\nsaid, “I’m going to swim today. You can’t swim because you’re working.” \\nTom said, “This isn’t work. I like painting.” \\n“Can I paint, too?” Ben asked. \\n“No, you can’t,” Tom answered. “Aunt Polly asked me \\nbecause I’m a very good painter.” \\nBen said, “I’m a good painter, too. Please, can I paint? I \\nhave some fruit. Do you want it?” \\nOK,” Tom said. “Give me the fruit. Then you can paint.” \\nBen started to paint the fence. Later, many boys came to \\nTom’s house. They watched Ben, and they wanted to paint, too. \\nTom said, “Give me some food and you can paint.” \\n \\n1 '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"The_Adventures_of_Tom_Sawyer.pdf\")\n",
    "document = loader.load()\n",
    "document[5].page_content[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(document, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마을 무덤에 있던 남자를 죽인 사람은 Injun Joe입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name = 'gpt-3.5-turbo',\n",
    "                 )\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "query = \"마을 무덤에 있던 남자를 죽인 사람은 누구니?\"\n",
    "result = qa({'query': query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lang Chain\n",
    "\n",
    "- 여러개의 모델을 하나로 연결하는 것\n",
    "\n",
    "### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "/opt/anaconda3/envs/llm/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'프랑스의 수도는 파리입니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                 model_name='gpt-4',\n",
    "                 )\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"{country}의 수도는 어디야?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"프랑스\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['sentence'],\n",
    "    template = \"다음 문장을 한글로 번역하세요. \\n\\n {sentence}\"\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key='translation')\n",
    "\n",
    "prompt2 = PromptTemplate.from_template(\n",
    "    \"다음 문장을 네 문장으로 요약하세요. \\n\\n {sentence}\"\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key='summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "all_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=['sentence'],\n",
    "    output_variables=['translation', 'summary'],\n",
    ")\n",
    "\n",
    "sentence=\"\"\"\n",
    "    The YOLOv10 paper introduces a new real-time end-to-end object detector that significantly advances the performance-efficiency boundary of the YOLO series. \n",
    "    The authors address the reliance on non-maximum suppression (NMS) for post-processing, which hinders end-to-end deployment and impacts inference latency. \n",
    "    They propose a consistent dual assignments strategy for NMS-free training, improving both performance and latency. Additionally, the paper introduces \n",
    "    a holistic efficiency-accuracy driven model design strategy, optimizing various components to reduce computational overhead and enhance capability. \n",
    "    Extensive experiments demonstrate that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales, \n",
    "    outperforming previous models in terms of computation-accuracy trade-offs.\n",
    "\"\"\"\n",
    "\n",
    "ans = all_chain(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '\\n    The YOLOv10 paper introduces a new real-time end-to-end object detector that significantly advances the performance-efficiency boundary of the YOLO series. \\n    The authors address the reliance on non-maximum suppression (NMS) for post-processing, which hinders end-to-end deployment and impacts inference latency. \\n    They propose a consistent dual assignments strategy for NMS-free training, improving both performance and latency. Additionally, the paper introduces \\n    a holistic efficiency-accuracy driven model design strategy, optimizing various components to reduce computational overhead and enhance capability. \\n    Extensive experiments demonstrate that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales, \\n    outperforming previous models in terms of computation-accuracy trade-offs.\\n',\n",
       " 'translation': 'YOLOv10 논문은 YOLO 시리즈의 성능-효율성 경계를 크게 향상시키는 새로운 실시간 종단간 객체 탐지기를 소개합니다. \\n저자들은 후처리를 위한 최대 비최대 억제(non-maximum suppression, NMS)에 대한 의존성을 다루며, 이는 종단간 배포를 방해하고 추론 지연에 영향을 미칩니다. \\n그들은 NMS-free 훈련을 위한 일관된 이중 할당 전략을 제안하며, 이는 성능과 지연 시간 모두를 향상시킵니다. 또한, 이 논문은 다양한 구성 요소를 최적화하여 \\n계산 오버헤드를 줄이고 능력을 향상시키는 종합적인 효율성-정확도 중심의 모델 설계 전략을 소개합니다. \\n다양한 모델 규모에서의 광범위한 실험은 YOLOv10이 최첨단 성능과 효율성을 달성하며, 계산-정확도 트레이드오프 측면에서 이전 모델들을 능가함을 보여줍니다.',\n",
       " 'summary': 'The YOLOv10 paper presents a new real-time object detector that improves the performance-efficiency of the YOLO series. The authors tackle the issue of reliance on non-maximum suppression (NMS) which affects end-to-end deployment and inference latency. They suggest a dual assignments strategy for NMS-free training, enhancing both performance and latency. The paper also introduces a comprehensive model design strategy, optimizing different components to lower computational overhead and increase capability.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The YOLOv10 paper presents a new real-time object detector that improves the performance-efficiency of the YOLO series. The authors tackle the issue of reliance on non-maximum suppression (NMS) which affects end-to-end deployment and inference latency. They suggest a dual assignments strategy for NMS-free training, enhancing both performance and latency. The paper also introduces a comprehensive model design strategy, optimizing different components to lower computational overhead and increase capability.\n"
     ]
    }
   ],
   "source": [
    "print(ans['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv10 논문은 YOLO 시리즈의 성능-효율성 경계를 크게 향상시키는 새로운 실시간 종단간 객체 탐지기를 소개합니다. \n",
      "저자들은 후처리를 위한 최대 비최대 억제(non-maximum suppression, NMS)에 대한 의존성을 다루며, 이는 종단간 배포를 방해하고 추론 지연에 영향을 미칩니다. \n",
      "그들은 NMS-free 훈련을 위한 일관된 이중 할당 전략을 제안하며, 이는 성능과 지연 시간 모두를 향상시킵니다. 또한, 이 논문은 다양한 구성 요소를 최적화하여 \n",
      "계산 오버헤드를 줄이고 능력을 향상시키는 종합적인 효율성-정확도 중심의 모델 설계 전략을 소개합니다. \n",
      "다양한 모델 규모에서의 광범위한 실험은 YOLOv10이 최첨단 성능과 효율성을 달성하며, 계산-정확도 트레이드오프 측면에서 이전 모델들을 능가함을 보여줍니다.\n"
     ]
    }
   ],
   "source": [
    "print(ans[\"translation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
